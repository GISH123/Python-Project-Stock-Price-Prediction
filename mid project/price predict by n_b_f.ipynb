{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "def get_data(days_delta = 1, threshold = 0.005 ):\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    target = dict()\n",
    "    file_market = open('market_ready.csv', 'r')\n",
    "    reader_market = csv.DictReader(file_market)\n",
    "    market_date_list = []\n",
    "\n",
    "\n",
    "    for a_row in reader_market:\n",
    "        date = datetime.strptime( a_row['date'], \"%Y-%m-%d\").date()\n",
    "        target[date]  = [-1]*5\n",
    "        r1 = float(a_row[u'1日報酬'])\n",
    "        r2 = float(a_row[u'2日報酬'])\n",
    "        r3 = float(a_row[u'3日報酬'])\n",
    "        r4 = float(a_row[u'4日報酬'])\n",
    "        r5 = float(a_row[u'5日報酬'])\n",
    "        return_list = [r1,r2,r3,r4,r5]\n",
    "        \n",
    "\n",
    "        market_date_list.append(date)\n",
    "        for i in range(5):\n",
    "            return_i = return_list[i]\n",
    "            if return_i < -1 * threshold : \n",
    "                target[date][i] = 0\n",
    "            elif return_i > threshold :\n",
    "                target[date][i] = 2\n",
    "            else:\n",
    "                target[date][i] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    time_delta = timedelta(hours = -13.5)\n",
    "    \n",
    "    df_list = []\n",
    "    for file_name in ['news_labeled.csv', 'bbs_labeled.csv', 'forum_labeled.csv']:\n",
    "        dict_for_number_of_up_and_down = dict() # 兩個值, 第一個代表label的漲跌相差數，第二個值代表總文章數\n",
    "        \n",
    "        if file_name.split('_')[0]=='news':\n",
    "            coding = 'big5'\n",
    "        else:\n",
    "            coding = 'utf-8'\n",
    "        with open( file_name, encoding = coding ) as f:\n",
    "\n",
    "            csv_reader = csv.DictReader(f)\n",
    "            for a_row in csv_reader:\n",
    "                if coding == 'utf-8':\n",
    "                    time = datetime.strptime( a_row['post_time'], \"%Y/%m/%d %H:%M:%S\") \n",
    "                else:\n",
    "                    time = datetime.strptime( a_row['post_time'], \"%Y/%m/%d %H:%M\") \n",
    "                adj_time = time+time_delta\n",
    "                adj_date = (adj_time + timedelta(days = days_delta )).date()\n",
    "                label = float(a_row['label'])\n",
    "                #cluster = a_row['cluster']\n",
    "\n",
    "\n",
    "                if  adj_date in market_date_list: \n",
    "\n",
    "                    try:\n",
    "                        dict_for_number_of_up_and_down[adj_date][0] += (label - 1)\n",
    "                        dict_for_number_of_up_and_down[adj_date][1] += 1  \n",
    "\n",
    "                    except:\n",
    "                        dict_for_number_of_up_and_down[adj_date] = [0, 0]\n",
    "        #standarize\n",
    "        standardize_value = dict()\n",
    "        for a_list, dates in zip(  dict_for_number_of_up_and_down.values(), dict_for_number_of_up_and_down.keys()   ): \n",
    "            if a_list[1] == 0:\n",
    "                standardize_value[dates] = [ 0., target[dates][days_delta-1]  ]\n",
    "            else:\n",
    "                standardize_value[dates] = [  a_list[0]/ a_list[1], target[dates][days_delta-1]  ]\n",
    "\n",
    "        #get df\n",
    "        df = pd.DataFrame.from_dict(standardize_value).T\n",
    "        df.columns = [ file_name.split('_')[0], 'target%i' %days_delta ]\n",
    "        df_list.append(df)\n",
    "        \n",
    "    df = pd.concat([df_list[1]['bbs'], df_list[0]], axis = 1, join='inner')\n",
    "    df = pd.concat([df_list[2]['forum'], df], axis = 1, join='inner')\n",
    "    return df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.000 0.5140363989573898\n",
      "1 0.001 0.3493334742705691\n",
      "1 0.002 0.36500277883244164\n",
      "1 0.003 0.31407975170251534\n",
      "1 0.004 0.30046125174734667\n",
      "1 0.005 0.3258464328407081\n",
      "2 0.000 0.5568905363247105\n",
      "2 0.001 0.329981787951565\n",
      "2 0.002 0.35069178008373864\n",
      "2 0.003 0.3355704473337994\n",
      "2 0.004 0.3299482939568132\n",
      "2 0.005 0.3496629230655863\n",
      "3 0.000 0.460760613645804\n",
      "3 0.001 0.3235683834193267\n",
      "3 0.002 0.31514271107050545\n",
      "3 0.003 0.2950746356772132\n",
      "3 0.004 0.34262457596051993\n",
      "3 0.005 0.3131642489434792\n",
      "4 0.000 0.47727594029575643\n",
      "4 0.001 0.3365049913803078\n",
      "4 0.002 0.30890155428991917\n",
      "4 0.003 0.32400103414491777\n",
      "4 0.004 0.3152529099554286\n",
      "4 0.005 0.3405550984789663\n",
      "5 0.000 0.48497183814655603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.001 0.31153004352748476\n",
      "5 0.002 0.2962837437589964\n",
      "5 0.003 0.2915799478891708\n",
      "5 0.004 0.31203712639424563\n",
      "5 0.005 0.34188295503080307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "max_score = 0\n",
    "for i in range(1,6):\n",
    "    for j in np.linspace(0, 0.005, num=6):\n",
    "        df_n_b_f = get_data(i, float(j) )\n",
    "\n",
    "        # RF\n",
    "        X = df_n_b_f.iloc[:,0:3]\n",
    "        y = df_n_b_f['target%i' %i]\n",
    "\n",
    "        clf =  RandomForestClassifier(criterion = 'entropy', n_estimators =12, random_state = 1, n_jobs = 4)\n",
    "\n",
    "        # scores\n",
    "\n",
    "        scores = cross_val_score(clf, X, y, cv= 6, scoring='f1_macro')\n",
    "        \n",
    "        print(i, '%0.3f' %float(j) , scores.mean())\n",
    "        \n",
    "        \n",
    "        if scores.mean() >  max_score:\n",
    "            max_score = scores.mean()\n",
    "            max_combination = ( i,float(j) )\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.41      0.34      0.37        62\n",
      "     class 2       0.56      0.63      0.59        82\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       144\n",
      "   macro avg       0.49      0.49      0.48       144\n",
      "weighted avg       0.50      0.51      0.50       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i, j = (1,0)\n",
    "df_n_b_f = get_data(i, float(j) )\n",
    "\n",
    "X = df_n_b_f.iloc[:,0:3]\n",
    "y = df_n_b_f['target%i' %i]\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split as tt_split\n",
    "x_train,x_test,y_train,y_test = tt_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "clf =  RandomForestClassifier(criterion = 'entropy', n_estimators =5, random_state = 1, n_jobs = 2)\n",
    "\n",
    "#fit\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = list(y_pred)\n",
    "y_true = list(y_test)\n",
    "target_names = ['class 0', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.55      0.42      0.47        72\n",
      "     class 2       0.53      0.66      0.59        73\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       145\n",
      "   macro avg       0.54      0.54      0.53       145\n",
      "weighted avg       0.54      0.54      0.53       145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i, j = (2,0)\n",
    "df_n_b_f = get_data(i, float(j) )\n",
    "\n",
    "X = df_n_b_f.iloc[:,0:3]\n",
    "y = df_n_b_f['target%i' %i]\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split as tt_split\n",
    "x_train,x_test,y_train,y_test = tt_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "clf =  RandomForestClassifier(criterion = 'entropy', n_estimators =5, random_state = 1, n_jobs = 2)\n",
    "\n",
    "#fit\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = list(y_pred)\n",
    "y_true = list(y_test)\n",
    "target_names = ['class 0', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 接下來我需要找到最好的分類器並超參優化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.000 0.5468493855547819\n",
      "Improve -0.0100 score\n",
      "2 0.000 0.5505138196184192\n",
      "Improve -0.0064 score\n",
      "2 0.000 0.5568905363247105\n",
      "Improve 0.0000 score\n",
      "2 0.000 0.5528395829233822\n",
      "2 0.000 0.5604090742855375\n",
      "Improve 0.0035 score\n",
      "2 0.000 0.5622043119974011\n",
      "Improve 0.0053 score\n",
      "2 0.000 0.5591095552266098\n",
      "2 0.000 0.5439009048229495\n",
      "2 0.000 0.5542809896815913\n",
      "2 0.000 0.5463281658107331\n",
      "2 0.000 0.5471638636538231\n",
      "2 0.000 0.5500421880439341\n",
      "2 0.000 0.5698645346032657\n",
      "Improve 0.0130 score\n",
      "2 0.000 0.5549965746648405\n",
      "2 0.000 0.5509068675786081\n",
      "2 0.000 0.5482226343783266\n",
      "2 0.000 0.5610978210978957\n",
      "2 0.000 0.5546629187103109\n",
      "2 0.000 0.5571005614105573\n",
      "2 0.000 0.543183657855261\n",
      "2 0.000 0.5486486077028379\n",
      "2 0.000 0.5376309967985496\n",
      "2 0.000 0.5443983019288486\n",
      "2 0.000 0.5408957603028156\n",
      "2 0.000 0.5417982744378043\n",
      "2 0.000 0.5335205760484185\n",
      "2 0.000 0.5416849490147722\n",
      "2 0.000 0.540527484641039\n",
      "2 0.000 0.5438132758337616\n",
      "2 0.000 0.5422362394700693\n",
      "2 0.000 0.5513934252896242\n",
      "2 0.000 0.5473594633386568\n",
      "2 0.000 0.5511538378320943\n",
      "2 0.000 0.5464790260128244\n",
      "2 0.000 0.55172127906462\n",
      "2 0.000 0.5447070808732543\n",
      "2 0.000 0.5494255576898297\n",
      "2 0.000 0.5425086308324364\n",
      "2 0.000 0.5430121883781349\n",
      "2 0.000 0.5499721516794387\n",
      "2 0.000 0.5494456353489738\n",
      "2 0.000 0.5456294460503202\n",
      "2 0.000 0.5547789508136246\n",
      "2 0.000 0.552142645988419\n",
      "2 0.000 0.5531194242654509\n",
      "2 0.000 0.5520968048538851\n",
      "2 0.000 0.5521524852908396\n",
      "2 0.000 0.5510598636122662\n",
      "2 0.000 0.5511304623934216\n",
      "2 0.000 0.5484687685694961\n",
      "2 0.000 0.5546010807939834\n",
      "2 0.000 0.5454703725088823\n",
      "2 0.000 0.5474628136942316\n",
      "2 0.000 0.5427997070680554\n",
      "2 0.000 0.5503000758835191\n",
      "2 0.000 0.543397394358821\n",
      "2 0.000 0.5472018852664954\n",
      "2 0.000 0.5401683522163578\n",
      "2 0.000 0.54053769556746\n",
      "2 0.000 0.5393920280357646\n",
      "2 0.000 0.5379187605152109\n",
      "2 0.000 0.5407650981528812\n",
      "2 0.000 0.5417032765472377\n",
      "2 0.000 0.5387683947040848\n",
      "2 0.000 0.5411405161615847\n",
      "2 0.000 0.53644030312965\n",
      "2 0.000 0.5402081384639505\n",
      "2 0.000 0.538514646290881\n",
      "2 0.000 0.5369599872026812\n",
      "2 0.000 0.5400275099214528\n",
      "2 0.000 0.5354417100234207\n",
      "2 0.000 0.5432157351441477\n",
      "2 0.000 0.5412175185787053\n",
      "2 0.000 0.5470932038385433\n",
      "2 0.000 0.5482409650090365\n",
      "2 0.000 0.5414499022775096\n",
      "2 0.000 0.5388468423242767\n",
      "2 0.000 0.5408194196139223\n",
      "2 0.000 0.5403122031727315\n",
      "2 0.000 0.5418682959829297\n",
      "2 0.000 0.5429088475788225\n",
      "2 0.000 0.5416384461302269\n",
      "2 0.000 0.5426920643503795\n",
      "2 0.000 0.5371995324573885\n",
      "2 0.000 0.5403460611518333\n",
      "2 0.000 0.5359780705339211\n",
      "2 0.000 0.5372405685932321\n",
      "2 0.000 0.5353965703332458\n",
      "2 0.000 0.5450822529113492\n",
      "2 0.000 0.5438235716459247\n",
      "2 0.000 0.5427331220331858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "base_score = 0.55689\n",
    "max_score = 0\n",
    "df_n_b_f = get_data(2, 0 )\n",
    "X = df_n_b_f.iloc[:,0:3]\n",
    "y = df_n_b_f['target%i' %i]\n",
    "for n in np.linspace(10, 100, num=91):\n",
    "    clf =  RandomForestClassifier(criterion = 'entropy', n_estimators =int(n), random_state = 1, n_jobs = 4)\n",
    "\n",
    "    # scores\n",
    "\n",
    "    scores = cross_val_score(clf, X, y, cv= 6, scoring='f1_macro')\n",
    "\n",
    "    print(i, '%0.3f' %float(j) , scores.mean())\n",
    "\n",
    "\n",
    "    if scores.mean() >  max_score:\n",
    "        \n",
    "        max_score = scores.mean()\n",
    "        max_combination = ( i,float(j) )\n",
    "        print('Improve %0.4f score' %(max_score-base_score) )\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_b_f = get_data(2, 0 )\n",
    "X = df_n_b_f.iloc[:,0:3]\n",
    "y = df_n_b_f['target%i' %i]\n",
    "x_train,x_test,y_train,y_test = tt_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 2500 candidates, totalling 15000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  41 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 767 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=4)]: Done 1985 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=4)]: Done 3683 tasks      | elapsed:   30.7s\n",
      "[Parallel(n_jobs=4)]: Done 5873 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=4)]: Done 8543 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 11705 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done 15000 out of 15000 | elapsed:  2.0min finished\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=6, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=2500, n_jobs=4,\n",
       "          param_distributions={'n_estimators': [10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30], 'criterion': ['gini', 'entropy'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 4, 6, 8, 10], 'min_samples_leaf': [1, 2, 4, 7, 10], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='f1_macro', verbose=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 30, num = 11)]\n",
    "\n",
    "# criterion to choose tree\n",
    "criterions = ['gini', 'entropy'] \n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 4, 6, 8, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 7, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'criterion' : criterions,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 2500, \n",
    "                               cv = 6, verbose=2, random_state=42, n_jobs = 4, scoring = 'f1_macro')\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 28,\n",
       " 'min_samples_split': 6,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_depth': 90,\n",
       " 'criterion': 'entropy',\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5236382858966038\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.48      0.38      0.42        72\n",
      "     class 2       0.49      0.60      0.54        73\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       145\n",
      "   macro avg       0.49      0.49      0.48       145\n",
      "weighted avg       0.49      0.49      0.48       145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf =  RandomForestClassifier(criterion = 'entropy', n_estimators = 28, max_depth = 90, min_samples_split = 6, random_state = 42, n_jobs = 4)\n",
    "\n",
    "scores = cross_val_score(clf, x_train, y_train, cv= 6, scoring='f1_macro')\n",
    "print(scores.mean())\n",
    "#fit\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = list(y_pred)\n",
    "y_true = list(y_test)\n",
    "target_names = ['class 0', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 莫名其妙得到很差的結果 不知道怎麼了???\n",
    "## 我想改提取關鍵字了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
